{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_by_lines(list_file, dir_path, list_filled):\n",
    "    \n",
    "    for labels in list_file:\n",
    "    \n",
    "        with open(dir_path + labels, \"r\") as file:\n",
    "\n",
    "            for readline in file: \n",
    "                line_strip = readline.strip()\n",
    "                \n",
    "                if line_strip == \"1\": \n",
    "                    list_filled.append(1) # unfair\n",
    "                elif line_strip == \"-1\":\n",
    "                    list_filled.append(0) # fair\n",
    "                else:\n",
    "                    list_filled.append(line_strip)\n",
    "\n",
    "    return list_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_label = \"data/Labels/\"\n",
    "dir_sentences = \"data/Sentences/\"\n",
    "\n",
    "list_file_labels = os.listdir(dir_label)\n",
    "list_file_sentences = os.listdir(dir_sentences)\n",
    "\n",
    "list_label = []\n",
    "list_sentences = []\n",
    "\n",
    "list_label = read_by_lines(list_file_labels, dir_label, list_label)\n",
    "list_sentences = read_by_lines(list_file_sentences, dir_sentences, list_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF LABELS: 9414\n",
      "NUMBER OF UNFAIR CASES: 1032\n",
      "NUMBER OF FAIR CASES: 8382\n"
     ]
    }
   ],
   "source": [
    "print(\"NUMBER OF LABELS:\", len(list_label))\n",
    "print(\"NUMBER OF UNFAIR CASES:\", list_label.count(1))\n",
    "print(\"NUMBER OF FAIR CASES:\", list_label.count(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF SENTENCES: 9414\n"
     ]
    }
   ],
   "source": [
    "print(\"NUMBER OF SENTENCES:\", len(list_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Convert Pandas to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'label':list_label, 'sentence':list_sentences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>* accepting the terms of service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>the purpose of this website , 9gag .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>com -lrb- the `` site '' -rrb- , owned and ope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>please read these terms of service -lrb- `` ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>by using or accessing the services , you agree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9409</th>\n",
       "      <td>0</td>\n",
       "      <td>you agree that given the unique and irreplacea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9410</th>\n",
       "      <td>0</td>\n",
       "      <td>therefore , for disputes that are not required...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9411</th>\n",
       "      <td>0</td>\n",
       "      <td>you agree to limit your claims to claims for m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9412</th>\n",
       "      <td>0</td>\n",
       "      <td>and , you agree not to seek injunctive or equi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9413</th>\n",
       "      <td>0</td>\n",
       "      <td>we are not liable for any changes or problems ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9414 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                           sentence\n",
       "0         0                   * accepting the terms of service\n",
       "1         0               the purpose of this website , 9gag .\n",
       "2         0  com -lrb- the `` site '' -rrb- , owned and ope...\n",
       "3         0  please read these terms of service -lrb- `` ag...\n",
       "4         1  by using or accessing the services , you agree...\n",
       "...     ...                                                ...\n",
       "9409      0  you agree that given the unique and irreplacea...\n",
       "9410      0  therefore , for disputes that are not required...\n",
       "9411      0  you agree to limit your claims to claims for m...\n",
       "9412      0  and , you agree not to seek injunctive or equi...\n",
       "9413      0  we are not liable for any changes or problems ...\n",
       "\n",
       "[9414 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.sentence.values\n",
    "y = data.label.values\n",
    "\n",
    "train_ratio = 0.75\n",
    "validation_ratio = 0.15\n",
    "test_ratio = 0.10\n",
    "\n",
    "# train is now 75% of the entire data set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 1 - train_ratio, random_state=42, stratify=y)\n",
    "\n",
    "# test is now 10% of the initial data set\n",
    "# validation is now 15% of the initial data set\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio), random_state=42, stratify=y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (7060,)\n",
      "Validation shape: (1412,)\n",
      "Test shape:  (942,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Validation shape:\", X_val.shape)\n",
    "print(\"Test shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt(\"train_x.txt\", X_train, delimiter=\" \", newline = \"\\n\", fmt=\"%s\")\n",
    "#np.savetxt(\"val_x.txt\", X_val, delimiter=\" \", newline = \"\\n\", fmt=\"%s\")\n",
    "#np.savetxt(\"test_x.txt\", X_test, delimiter=\" \", newline = \"\\n\", fmt=\"%s\")\n",
    "\n",
    "#np.savetxt(\"train_y.txt\", y_train, delimiter=\" \", newline = \"\\n\", fmt=\"%s\")\n",
    "#np.savetxt(\"val_y.txt\", y_val, delimiter=\" \", newline = \"\\n\", fmt=\"%s\")\n",
    "#np.savetxt(\"test_y.txt\", y_test, delimiter=\" \", newline = \"\\n\", fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():       \n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fine Tuning Legal-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "stemer = PorterStemmer()\n",
    "\n",
    "special_chars = re.compile('[^⁰9a-z#+_]')\n",
    "add_space = re.compile('[/(){}\\[\\]\\\\@;]')\n",
    "                             \n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = add_space.sub(' ',text)\n",
    "    text = special_chars.sub(' ',text)\n",
    "    \n",
    "    return re.sub(' +', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  com -lrb- the `` site '' -rrb- , owned and operated by 9gag , inc , is to provide web publishing services .\n",
      "Processed:  com lrb the site rrb owned and operated by 9gag inc is to provide web publishing services \n"
     ]
    }
   ],
   "source": [
    "# Print sentence 0\n",
    "print('Original: ', X[2])\n",
    "print('Processed: ', clean_text(X[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('nlpaueb/legal-bert-small-uncased', do_lower_case=True)\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(data):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    data (np.array): Array of texts to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in data:\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentence\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
    "        #    (3) Truncate/Pad sentence to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention mask\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=clean_text(sent),  # Preprocess sentence\n",
    "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
    "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
    "            pad_to_max_length=True,         # Pad sentence to max length\n",
    "            #return_tensors='pt',           # Return PyTorch tensor\n",
    "            return_attention_mask=True      # Return attention mask\n",
    "            )\n",
    "        \n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (598 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length:  598\n"
     ]
    }
   ],
   "source": [
    "all_sentences = data.sentence.values\n",
    "\n",
    "# Encode our concatenated data\n",
    "encoded_sentences = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_sentences]\n",
    "\n",
    "# Find the maximum length\n",
    "max_len = max([len(sent) for sent in encoded_sentences])\n",
    "print('Max length: ', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  * accepting the terms of service\n",
      "Token IDs:  [101, 1599, 235, 207, 333, 210, 446, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Tokenizing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\merve\\Anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "# Specify `MAX_LEN`\n",
    "MAX_LEN = 64\n",
    "\n",
    "# Print sentence 0 and its encoded token ids\n",
    "token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n",
    "print('Original: ', X[0])\n",
    "print('Token IDs: ', token_ids)\n",
    "\n",
    "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
    "print('Tokenizing data...')\n",
    "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
    "val_inputs, val_masks = preprocessing_for_bert(X_val)\n",
    "test_inputs, test_masks = preprocessing_for_bert(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Create PyTorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Convert other data types to torch.Tensor\n",
    "train_labels = torch.tensor(y_train)\n",
    "val_labels = torch.tensor(y_val)\n",
    "test_labels = torch.tensor(y_test)\n",
    "\n",
    "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Create the DataLoader for our training set\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set\n",
    "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our test set\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Create BertClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlpaueb/legal-bert-small-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-small-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'nlpaueb/legal-bert-small-uncased',\n",
    "    num_labels = 2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Optimizer & Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
    "    \"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = model\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = AdamW(bert_classifier.parameters(),\n",
    "                      lr=5e-5,    # Default learning rate\n",
    "                      eps=1e-8    # Default epsilon value\n",
    "                      )\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0, # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits['logits'], b_labels)\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits['logits'], b_labels)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = torch.argmax(logits['logits'], dim=1).flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\merve\\Anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.374288   |     -      |     -     |   58.15  \n",
      "   1    |   40    |   0.253047   |     -      |     -     |   61.05  \n",
      "   1    |   60    |   0.254835   |     -      |     -     |   62.07  \n",
      "   1    |   80    |   0.215781   |     -      |     -     |   64.53  \n",
      "   1    |   100   |   0.159577   |     -      |     -     |   61.85  \n",
      "   1    |   120   |   0.159250   |     -      |     -     |   60.83  \n",
      "   1    |   140   |   0.200599   |     -      |     -     |   61.60  \n",
      "   1    |   160   |   0.163076   |     -      |     -     |   62.94  \n",
      "   1    |   180   |   0.176827   |     -      |     -     |   62.14  \n",
      "   1    |   200   |   0.177963   |     -      |     -     |   64.24  \n",
      "   1    |   220   |   0.156118   |     -      |     -     |   61.75  \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.209056   |  0.151603  |   93.96   |  724.24  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=1)\n",
    "train(bert_classifier, train_dataloader, val_dataloader, epochs=1, evaluation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def bert_predict(model, test_dataloader):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "        all_logits.append(logits['logits'])\n",
    "    \n",
    "    # Concatenate logits from each batch\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "\n",
    "    # Apply softmax to calculate probabilities\n",
    "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "\n",
    "def evaluate_roc(probs, y_true):\n",
    "    \"\"\"\n",
    "    - Print AUC and accuracy on the test set\n",
    "    - Plot ROC\n",
    "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
    "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
    "    \"\"\"\n",
    "    preds = probs[:, 1]\n",
    "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f'AUC: {roc_auc:.4f}')\n",
    "       \n",
    "    # Get accuracy over the test set\n",
    "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    # Plot ROC AUC\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9380\n",
      "Accuracy: 92.89%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute predicted probabilities on the test set\n",
    "probs = bert_predict(bert_classifier, test_dataloader)\n",
    "\n",
    "# Evaluate the Bert classifier\n",
    "evaluate_roc(probs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bert_classifier.state_dict(), 'legalSeqBertFineTuned_Claudette_%93_acc.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
