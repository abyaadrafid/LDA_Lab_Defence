{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LegalBert4SeqClassif_ModelEnhancement.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyOvP6vSXJ4ZnIvY/D9ArMFq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# ***Legal BERT - Model Enhancement***"],"metadata":{"id":"7pdKG_0kdKNY"}},{"cell_type":"markdown","source":["### Global variables"],"metadata":{"id":"ph2Sd0V0f8zA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OtMDzI63cxXF"},"outputs":[],"source":["# Global variables\n","\n","BATCH_SIZE = 32\n","MODEL_NAME = 'nlpaueb/legal-bert-small-uncased' #'bert-base-uncased'\n","EPOCHS = 3\n","EMBEDDING_SIZE = 512\n","NUM_CLASSES = 2\n","VOCABULARY_SIZE = 30522\n","NUM_TOKENS = 10\n","DEFAULT_TOKEN = 207 # What in BERT represents the word \"the\"\n","LIST_ID_SPECIAL_TOKENS = [0, 101, 102, 103]\n","LIST_SPECIAL_TOKENS = ['[PAD]', '[CLS]', '[SEP]', '[MASK]']"]},{"cell_type":"markdown","source":["### Installation of packages"],"metadata":{"id":"GkoBzkPfgGbV"}},{"cell_type":"code","source":["!pip install transformers\n","!pip install torch-lr-finder"],"metadata":{"id":"yvLTD3JcgJjj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Imports"],"metadata":{"id":"qTV9i90-gMrK"}},{"cell_type":"code","source":["import torch\n","import os\n","from transformers import BertTokenizer\n","from google.colab import drive\n","from torch.utils.data import TensorDataset, random_split\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from transformers import get_linear_schedule_with_warmup\n","import numpy as np\n","import time\n","import datetime\n","import random\n","import gc\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n","from sklearn.model_selection import train_test_split\n","from copy import deepcopy\n","from datetime import datetime\n","import datetime"],"metadata":{"id":"sr1sOPFfgOC1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Mount on GoogleDrive"],"metadata":{"id":"jH9Kb1j6gski"}},{"cell_type":"code","source":["drive.mount('/content/drive')"],"metadata":{"id":"ieb8-zc_gvw9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Device"],"metadata":{"id":"_vK3qX7DgQWi"}},{"cell_type":"code","source":["if torch.cuda.is_available():    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"metadata":{"id":"5Bdcy5UAgRVf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Reading original dataset"],"metadata":{"id":"osD8Dk0mgjmd"}},{"cell_type":"code","source":["def get_sentences(path):\n","    sentences= []\n","    for filename in sorted(os.listdir(path)):\n","        with open(path+filename, 'r') as f:\n","            for sentence in f :\n","                if sentence.strip() != '':\n","                    sentences.append(sentence)\n","    return sentences\n","\n","def get_labels(path):\n","    all_labels = []\n","    for filename in sorted(os.listdir(path)):\n","        file_labels = []\n","        with open(path+filename, 'r') as f:\n","            for label in f :\n","                if label.strip() != '':\n","                    all_labels.append(int(label))\n","    return all_labels"],"metadata":{"id":"DudLv10rgX8t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Reading sentences and labels of the Claudette Dataset\n","all_sentences = get_sentences(\"ToS/TrainValSet/Sentences/\")\n","all_labels = get_labels(\"ToS/TrainValSet/Labels/\")\n","\n","# Conversion of unfair sentence's flag|tag. Since unfair sentences are marked as \"-1\", we change them to \"0\" \n","all_labels =  [0 if label ==-1 else label for label in all_labels]"],"metadata":{"id":"-Mm3CtQmhKZw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Trigger Generation**"],"metadata":{"id":"GmMyL6l6hgfi"}},{"cell_type":"markdown","source":["### Bert (Model, Tokenizer and Load)"],"metadata":{"id":"VojPVT9why_j"}},{"cell_type":"code","source":["print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=True) # the model 'bert-base-uncased' only contains lower case sentences"],"metadata":{"id":"K7BsbevuhvaT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model BertForSequenceClassification (Load model)"],"metadata":{"id":"M7vhHZYAi7sm"}},{"cell_type":"code","source":["model = BertForSequenceClassification.from_pretrained(\n","    MODEL_NAME,\n","    num_labels = NUM_CLASSES,\n","    output_attentions = False,\n","    output_hidden_states = False,\n",")\n","\n","model.cuda()"],"metadata":{"id":"-2fcQVw-i1eb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.load_state_dict(torch.load('Bert4SeqClassif_20220804_1134_wPersistency.pt'))"],"metadata":{"id":"DLgYcXdvjH37"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Trigger generation"],"metadata":{"id":"6ov6kgpbjZF1"}},{"cell_type":"markdown","source":["##### General functions"],"metadata":{"id":"fHepLK4AkHD_"}},{"cell_type":"code","source":["# hook used in add_hooks()\n","extracted_grads = []\n","def extract_grad_hook(module, grad_in, grad_out):\n","    extracted_grads.append(grad_out[0])\n","\n","# returns the wordpiece embedding weight matrix\n","def get_embedding_weight(language_model):\n","    for module in language_model.modules():\n","        if isinstance(module, torch.nn.Embedding):\n","            if module.weight.shape[0] == 30522: \n","                return module.weight.detach()\n","\n","# add hooks for embeddings\n","def add_hooks(language_model):\n","    for module in language_model.modules():\n","        if isinstance(module, torch.nn.Embedding):\n","            if module.weight.shape[0] == 30522: \n","                module.weight.requires_grad = True\n","                module.register_full_backward_hook(extract_grad_hook)\n","\n","\n","# creates the batch of target texts with -1 placed at the end of the sequences for padding (for masking out the loss).\n","def make_target_batch(tokenizer, device, target_texts):\n","    # encode items and get the max length\n","    encoded_texts = []\n","    max_len = 0\n","    for target_text in target_texts:\n","        encoded_target_text = tokenizer.encode_plus(\n","            target_text,\n","            add_special_tokens = True,\n","            max_length = EMBEDDING_SIZE - NUM_TOKENS,\n","            pad_to_max_length = True,\n","            return_attention_mask = True\n","        )\n","        encoded_texts.append(encoded_target_text.input_ids)\n","        if len(encoded_target_text.input_ids) > max_len:\n","            max_len = len(encoded_target_text)\n","\n","    # pad tokens, i.e., append -1 to the end of the non-longest ones\n","    for indx, encoded_text in enumerate(encoded_texts):\n","        if len(encoded_text) < max_len:\n","            encoded_texts[indx].extend([-1] * (max_len - len(encoded_text)))\n","\n","    # convert to tensors and batch them up\n","    target_tokens_batch = None\n","    for encoded_text in encoded_texts:\n","        target_tokens = torch.tensor(encoded_text, device=device, dtype=torch.long).unsqueeze(0)\n","        if target_tokens_batch is None:\n","            target_tokens_batch = target_tokens\n","        else:\n","            target_tokens_batch = torch.cat((target_tokens, target_tokens_batch), dim=0)\n","    return target_tokens_batch\n","\n","# Got from https://github.com/Eric-Wallace/universal-triggers/blob/master/attacks.py\n","def hotflip_attack(averaged_grad, embedding_matrix, trigger_token_ids,\n","                   increase_loss=False, num_candidates=1):\n","    \"\"\"\n","    The \"Hotflip\" attack described in Equation (2) of the paper. This code is heavily inspired by\n","    the nice code of Paul Michel here https://github.com/pmichel31415/translate/blob/paul/\n","    pytorch_translate/research/adversarial/adversaries/brute_force_adversary.py\n","    This function takes in the model's average_grad over a batch of examples, the model's\n","    token embedding matrix, and the current trigger token IDs. It returns the top token\n","    candidates for each position.\n","    If increase_loss=True, then the attack reverses the sign of the gradient and tries to increase\n","    the loss (decrease the model's probability of the true class). For targeted attacks, you want\n","    to decrease the loss of the target class (increase_loss=False).\n","    \"\"\"\n","    averaged_grad = averaged_grad.cpu()\n","    embedding_matrix = embedding_matrix.cpu()\n","    trigger_token_embeds = torch.nn.functional.embedding(torch.LongTensor(trigger_token_ids),\n","                                                         embedding_matrix).detach().unsqueeze(0)\n","    averaged_grad = averaged_grad.unsqueeze(0)\n","    gradient_dot_embedding_matrix = torch.einsum(\"bij,kj->bik\",\n","                                                 (averaged_grad, embedding_matrix))        \n","    if not increase_loss:\n","        gradient_dot_embedding_matrix *= -1    # lower versus increase the class probability.\n","    if num_candidates > 1: # get top k options\n","        _, best_k_ids = torch.topk(gradient_dot_embedding_matrix, num_candidates, dim=2)\n","        return best_k_ids.detach().cpu().numpy()[0]\n","    _, best_at_each_step = gradient_dot_embedding_matrix.max(2)\n","    return best_at_each_step[0].detach().cpu().numpy()\n","\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"metadata":{"id":"xTpEhRQmjNEQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_input_masks_and_labels_with_tokens(sentences, labels, tokens, position='B'):\n","    input_ids = []\n","    attention_masks = []\n","    number_of_tokens = []\n","\n","    for sent in sentences:\n","\n","        if position == 'B':\n","            sent_with_tokens = tokens + \" \" + sent\n","        elif position == 'E':\n","            sent_with_tokens = sent + \" \" + tokens\n","        else:\n","            print('Wrong position command, please enter \"E\" or \"B\"')\n","            return\n","\n","        encoded_dict = tokenizer.encode_plus(\n","                        sent_with_tokens,\n","                        add_special_tokens = True,\n","                        max_length = 512,\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,\n","                        return_tensors = 'pt',\n","                   )\n","\n","\n","        input_ids.append(encoded_dict['input_ids'])\n","\n","        attention_masks.append(encoded_dict['attention_mask'])\n","\n","    input_ids = torch.cat(input_ids, dim=0)\n","    attention_masks = torch.cat(attention_masks, dim=0)\n","    labels = torch.tensor(labels)\n","\n","    # count number of tokens of each sentence\n","    for idx in range(len(input_ids)):\n","      sent_ids = input_ids[idx, :]\n","\n","      cnt = 0\n","      for id in sent_ids:\n","          if id != 0:\n","              cnt += 1\n","\n","      number_of_tokens.append(cnt)  \n","\n","    return input_ids, attention_masks, labels, number_of_tokens"],"metadata":{"id":"eiuMnLcZkR2j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_loss_and_metrics(model, dataloader, device):\n","    # get initial loss for the trigger\n","    model.zero_grad()\n","\n","    test_preds = []\n","    test_targets = []\n","\n","    # Tracking variables \n","    total_test_accuracy = 0\n","    total_test_loss = 0\n","    io_total_test_acc = 0\n","    io_total_test_prec = 0\n","    io_total_test_recall = 0\n","    io_total_test_f1 = 0\n","\n","    for batch in dataloader:\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        model.zero_grad()\n","\n","        result = model(b_input_ids, \n","                    token_type_ids=None, \n","                    attention_mask=b_input_mask, \n","                    labels=b_labels,\n","                    return_dict=True)\n","\n","        loss = result.loss\n","        logits = result.logits\n","\n","        test_preds.extend(logits.argmax(dim=1).cpu().numpy())\n","        test_targets.extend(batch[2].numpy())\n","\n","        # Accumulate the validation loss.\n","        total_test_loss += loss.item()\n","\n","        test_preds.extend(logits.argmax(dim=1).cpu().numpy())\n","        test_targets.extend(batch[2].numpy())\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        loss.backward()        \n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.        \n","        test_acc = accuracy_score(test_targets, test_preds)\n","        test_precision = precision_score(test_targets, test_preds)\n","        test_recall = recall_score(test_targets, test_preds)\n","        test_f1 = f1_score(test_targets, test_preds)\n","\n","        io_total_test_acc += test_acc\n","        io_total_test_prec += test_precision\n","        io_total_test_recall += test_recall\n","        io_total_test_f1 += test_f1\n","\n","    io_avg_test_loss = total_test_loss/len(dataloader)\n","    io_avg_test_acc = io_total_test_acc / len(dataloader)\n","    io_avg_test_prec = io_total_test_prec / len(dataloader)\n","    io_avg_test_recall = io_total_test_recall / len(dataloader)\n","    io_avg_test_f1 = io_total_test_f1 / len(dataloader)\n","\n","    return io_avg_test_loss, io_avg_test_acc, io_avg_test_prec, io_avg_test_recall, io_avg_test_f1"],"metadata":{"id":"cVi1JG1ekMkL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def change_input_ids_with_candidate_token(input_ids, position, candidate, number_of_tokens, trigger_position='B'):\n","    if trigger_position == 'B':\n","        input_ids[:, position] = candidate\n","    elif trigger_position == 'E':\n","        for idx in range(len(input_ids)):\n","            if number_of_tokens[idx] > EMBEDDING_SIZE:\n","                input_ids[idx, EMBEDDING_SIZE-NUM_TOKENS-2+position] = candidate\n","            else:\n","                input_ids[idx, number_of_tokens[idx]-NUM_TOKENS-2+position] = candidate\n","    else:\n","        print('Wrong position command, please enter \"E\" or \"B\"')\n","        return\n","    return input_ids"],"metadata":{"id":"yoPr3yJe1xrG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Execution of the trigger generation"],"metadata":{"id":"XJsvcJua2FVP"}},{"cell_type":"markdown","source":["###### Get unfair sentences"],"metadata":{"id":"G4fBirI72W6s"}},{"cell_type":"code","source":["positions_unfair = np.where(np.array(all_labels) == 1)[0]\n","\n","target_unfair_sentences = []\n","labels_unfair_sentences = []\n","for index in range(len(positions_unfair)):\n","    target_unfair_sentences.append(all_sentences[positions_unfair[index]])\n","    labels_unfair_sentences.append(all_labels[positions_unfair[index]])\n","\n","# Initialization of tokens with the particle \"the\" whose id number in BERT model is 207\n","trigger_tokens = np.array([DEFAULT_TOKEN]*NUM_TOKENS)\n","print(tokenizer.decode(trigger_tokens))"],"metadata":{"id":"QTKzAMth1-64"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### Addition of hooks and get word embeddings"],"metadata":{"id":"7G6pl7nF2vCB"}},{"cell_type":"code","source":["model.eval()\n","model.to(device)\n","\n","add_hooks(model) # add gradient hooks to embeddings\n","embedding_weight = get_embedding_weight(model) # save the word embedding matrix"],"metadata":{"id":"hUPKtMir24Ww"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### Get tensors and creation of dataset and dataloader"],"metadata":{"id":"IBgwSKMk275K"}},{"cell_type":"code","source":["## Define at which position we want to have the triggers\n","position = 'B' # Possible values {B|E} for beginning and end, respectively\n","\n","input_ids, attention_masks, labels, number_of_tokens = get_input_masks_and_labels_with_tokens(target_unfair_sentences, labels_unfair_sentences, tokenizer.decode(trigger_tokens), position=position)\n","\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE)"],"metadata":{"id":"yv4m7W-w3LQb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###### Execution of trigger generation"],"metadata":{"id":"qH9EPKm73or2"}},{"cell_type":"code","source":["timestamp = datetime.datetime.now().strftime(\"%y%m%d_%H_%M_%S_%p\")\n","f = open(f\"Execution_Pos{position}_NumTokens_{NUM_TOKENS}_{timestamp}.txt\", \"w\")"],"metadata":{"id":"qJc-5gdiRmpu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["candidates_dict = {}\n","candidates_dict[\"combinations_ids\"] = []\n","candidates_dict[\"combinations_labels\"] = []\n","foundX = -1\n","foundY = -1\n","candidates_combination = [DEFAULT_TOKEN]*NUM_TOKENS\n","goalTag = 'Loss'\n","\n","extracted_grads = []\n","\n","loss_obtained, acc_obtained, prec_obtained, recall_obtained, f1_obtained = get_loss_and_metrics(model, dataloader, device)\n","#print(f'loss_obtained {loss_obtained}')\n","\n","candidates_selected = [DEFAULT_TOKEN]*NUM_TOKENS\n","# try all the candidates and pick the best\n","curr_best_loss = loss_obtained\n","curr_best_trigger_tokens = None\n","\n","print(f\"{position}[{foundX:3},{foundY:3}] TokenID[{candidates_combination}] TokensDesc[{tokenizer.decode(candidates_combination)}] Loss[{loss_obtained:3.5}] Acc[{acc_obtained:3.5}] Prec[{prec_obtained:3.5}] Recall[{recall_obtained:3.5}] F1[{f1_obtained:3.5}] => Worst<<{goalTag}>>[{curr_best_loss:3.5}] Found at [{foundX:3},{foundY:3}]\")\n","f.write(f'{position}[{foundX:3},{foundY:3}] TokenID[{candidates_combination}] TokensDesc[{tokenizer.decode(candidates_combination)}] Loss[{loss_obtained:3.5}] Acc[{acc_obtained:3.5}] Prec[{prec_obtained:3.5}] Recall[{recall_obtained:3.5}] F1[{f1_obtained:3.5}] => Worst<<{goalTag}>>[{curr_best_loss:3.5}] Found at [{foundX:3},{foundY:3}]')\n","\n","for id_token_to_flip in range(0, NUM_TOKENS):\n","\n","    averaged_grad = torch.sum(extracted_grads[0], dim=0)\n","    averaged_grad = averaged_grad[id_token_to_flip].unsqueeze(0)\n","\n","    # Use hotflip (linear approximation) attack to get the top num_candidates\n","    candidates = hotflip_attack(averaged_grad, embedding_weight,\n","                                        [trigger_tokens[id_token_to_flip]], \n","                                        increase_loss=False, num_candidates=100)[0]\n","    print(f'candidates {candidates}')\n","    f.write(f'{position} candidates {candidates}')\n","    candidates_dict[id_token_to_flip] = candidates\n","    \n","    for index, cand in enumerate(candidates):\n","        extracted_grads = []\n","\n","        if cand in LIST_ID_SPECIAL_TOKENS:\n","          continue\n","\n","        input_ids_with_candidate_trigger = change_input_ids_with_candidate_token(deepcopy(input_ids), id_token_to_flip+1, cand, number_of_tokens, trigger_position=position)\n","        dataset_with_candidate_trigger = TensorDataset(input_ids_with_candidate_trigger, attention_masks, labels)\n","        dataloader_with_candidate_trigger = torch.utils.data.DataLoader(dataset_with_candidate_trigger, batch_size=BATCH_SIZE)\n","\n","        current_loss, current_acc, current_prec, current_recall, current_f1 = get_loss_and_metrics(model, dataloader_with_candidate_trigger, device)\n","\n","        if curr_best_loss < current_loss:\n","            curr_best_loss = current_loss\n","            candidates_selected[id_token_to_flip] = cand\n","\n","            foundX = id_token_to_flip\n","            foundY = index\n","        candidates_combination[id_token_to_flip] = cand\n","        candidates_dict[\"combinations_ids\"].append(candidates_combination)\n","        candidates_dict[\"combinations_labels\"].append(tokenizer.decode(candidates_combination))\n","        print(f'[{id_token_to_flip:3},{index:3}] TokenID[{candidates_combination}] TokensDesc[{tokenizer.decode(candidates_combination)}] Loss[{current_loss:3.5}] Acc[{current_acc:3.5}] Prec[{current_prec:3.5}] Recall[{current_recall:3.5}] F1[{current_f1:3.5}] => Worst<<{goalTag}>>[{curr_best_loss:3.5}] Found at [{foundX:3},{foundY:3}]')\n","        f.write(f'{position}[{id_token_to_flip:3},{index:3}] TokenID[{candidates_combination}] TokensDesc[{tokenizer.decode(candidates_combination)}] Loss[{current_loss:3.5}] Acc[{current_acc:3.5}] Prec[{current_prec:3.5}] Recall[{current_recall:3.5}] F1[{current_f1:3.5}] => Worst<<{goalTag}>>[{curr_best_loss:3.5}] Found at [{foundX:3},{foundY:3}]\\n')\n","        \n","        del input_ids_with_candidate_trigger\n","        del dataset_with_candidate_trigger\n","        del dataloader_with_candidate_trigger\n","\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","\n","        #print(f'Candidates selected {candidates_selected} VS candidates_combination {candidates_combination}')\n","        #print(f'[{id_token_to_flip}][{index}] loss[{index}] {current_loss} ({curr_best_loss})')\n","\n","    candidates_combination = deepcopy(candidates_selected)\n","    #extracted_grads = []\n","    input_ids = change_input_ids_with_candidate_token(deepcopy(input_ids), id_token_to_flip+1, candidates_selected[id_token_to_flip], number_of_tokens, trigger_position=position)\n","    print(f'Worst loss {curr_best_loss} with candidates {candidates_selected} in the {id_token_to_flip}-iteration with tokens [{tokenizer.decode(candidates_selected)}]')\n","    f.write(f'{position}Worst loss {curr_best_loss} with candidates {candidates_selected}\\n')\n","f.close()"],"metadata":{"id":"sG9jGWwL3t7d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.decode(candidates_selected)"],"metadata":{"id":"9UkDXrIG0IiM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Generate sentences with tokens"],"metadata":{"id":"jTdsbDJapDkf"}},{"cell_type":"code","source":["len(target_unfair_sentences)"],"metadata":{"id":"w2-_S4Yr1q8H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Store new sentences with the tokens at the beginning\n","position=\"B\"\n","list_tokens_deal_worst_loss = [\n","    tokenizer.decode([621, 207, 3523]), #unless the everyone\n","    tokenizer.decode([621, 207, 207]) #unless the the\n","]\n","\n","dict_distribution = {\n","    0: 0,\n","    1: 0\n","}\n","\n","timestamp = datetime.datetime.now().strftime(\"%y%m%d_%H_%M_%S_%p\")\n","f = open(f\"ToS/DataAugmentation/Pos{position}_NumTokens{NUM_TOKENS}_{timestamp}.txt\", \"w\")\n","\n","for unf_sent in target_unfair_sentences:\n","    index = 1 if random.uniform(0,1) > 0.5 else 0\n","    f.write(f'{list_tokens_deal_worst_loss[index]} {unf_sent}\\n')\n","    dict_distribution[index] += 1\n","f.close()\n","\n","print(dict_distribution) # {0: 508, 1: 524} => {0: 471, 1: 520} (96(95-05)-04))"],"metadata":{"id":"eADqdnKAh3e6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Store new sentences with the tokens at the beginning\n","position=\"E\"\n","list_tokens_deal_worst_loss = [\n","    tokenizer.decode([1297, 17560, 1004]), #unless the everyone\n","    tokenizer.decode([1297, 17560, 431]) #unless the the\n","]\n","\n","dict_distribution = {\n","    0: 0,\n","    1: 0\n","}\n","\n","timestamp = datetime.datetime.now().strftime(\"%y%m%d_%H_%M_%S_%p\")\n","f = open(f\"ToS/DataAugmentation/Pos{position}_NumTokens{NUM_TOKENS}_{timestamp}.txt\", \"w\")\n","\n","for unf_sent in target_unfair_sentences:\n","    index = 1 if random.uniform(0,1) > 0.5 else 0\n","\n","    sent_tokenized = tokenizer.encode(unf_sent)\n","\n","    if len(sent_tokenized) > 507:\n","      str_aux = (tokenizer.decode(sent_tokenized[0:507])).replace(\"\\n\", \"\")\n","      f.write(f'{str_aux} {list_tokens_deal_worst_loss[index]}\\n')\n","    else:\n","      str_aux = unf_sent.replace(\"\\n\", \"\")\n","      f.write(f'{str_aux} {list_tokens_deal_worst_loss[index]}\\n')\n","    dict_distribution[index] += 1\n","f.close()\n","\n","print(dict_distribution) "],"metadata":{"id":"2ESyo77Jl1s6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training of a new model"],"metadata":{"id":"2mthIUQypWg0"}},{"cell_type":"markdown","source":["### Data split"],"metadata":{"id":"m9POhis6qbP_"}},{"cell_type":"code","source":["all_sentences_and_augmented = get_sentences(\"ToS/TrainValSetWithAugmentedData/Sentences/\")\n","all_labels_and_augmented = get_labels(\"ToS/TrainValSetWithAugmentedData/Labels/\")"],"metadata":{"id":"Hln8HJa0-zwX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids = []\n","attention_masks = []\n","\n","for sent in all_sentences_and_augmented:\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,\n","                        add_special_tokens = True,\n","                        max_length = 512,         \n","                        pad_to_max_length = True, \n","                        return_attention_mask = True,\n","                        return_tensors = 'pt',    \n","                   )\n","    \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(all_labels_and_augmented)\n","\n","print('Original: ', all_sentences_and_augmented[0])\n","print('Token IDs:', input_ids[0])\n","\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","\n","train_idx, valid_idx = train_test_split(np.arange(len(labels)), test_size=0.05, shuffle=True, stratify=labels)\n","\n","train_sampler = torch.utils.data.SubsetRandomSampler(train_idx)\n","valid_sampler = torch.utils.data.SubsetRandomSampler(valid_idx)\n","\n","train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n","validation_dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, sampler=valid_sampler)"],"metadata":{"id":"VDtPTWVzp_mc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training classification model"],"metadata":{"id":"E4HVDjX_sSyx"}},{"cell_type":"code","source":["model = BertForSequenceClassification.from_pretrained(\n","    MODEL_NAME,\n","    num_labels = NUM_CLASSES,\n","    output_attentions = False,\n","    output_hidden_states = False,\n",")\n","model.cuda()"],"metadata":{"id":"F3kCP4P3sPSM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Optimizer & Learning Rate Scheduler"],"metadata":{"id":"Y5KcpRZDsZWY"}},{"cell_type":"code","source":["\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5,\n","                  eps = 1e-8,\n","                )"],"metadata":{"id":"5KmzLrDfsVGS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = EPOCHS\n","\n","total_steps = len(train_dataloader) * epochs\n","\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","MIN_LR = 1e-5\n","scheduler = CosineAnnealingLR(optimizer, 600, eta_min = MIN_LR)"],"metadata":{"id":"GdsFUHWPsdOn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training"],"metadata":{"id":"S6SLYPm-sk46"}},{"cell_type":"code","source":["def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","def format_time(elapsed):\n","    elapsed_rounded = int(round((elapsed)))\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"],"metadata":{"id":"PmyBdJ_xsgna"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tr_metrics = []\n","va_metrics = []\n","tmp_print_flag = True\n","\n","\n","# This training code is based on the `run_glue.py` script here:\n","# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n","\n","# Set the seed value all over the place to make this reproducible.\n","seed_val = 42\n","\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss, \n","# validation accuracy, and timings.\n","training_stats = []\n","\n","# Measure the total training time for the whole run.\n","total_t0 = time.time()\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","    train_loss = 0.0\n","    train_preds = []\n","    train_targets = []\n","    \n","    # ========================================\n","    #               Training\n","    # ========================================\n","    \n","    # Perform one full pass over the training set.\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes.\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch.\n","    total_train_loss = 0\n","\n","    io_total_train_acc = 0\n","    io_total_train_prec = 0\n","    io_total_train_recall = 0\n","    io_total_train_f1 = 0\n","    io_total_valid_acc = 0\n","    io_total_valid_prec = 0\n","    io_total_valid_recall = 0\n","    io_total_valid_f1 = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to \n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 100 batches.\n","        if step % 100 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes.\n","            elapsed = format_time(time.time() - t0)\n","            \n","            # Report progress.\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","        # `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because \n","        # accumulating the gradients is \"convenient while training RNNs\". \n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()        \n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # In PyTorch, calling `model` will in turn call the model's `forward` \n","        # function and pass down the arguments. The `forward` function is \n","        # documented here: \n","        # https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification\n","        # The results are returned in a results object, documented here:\n","        # https://huggingface.co/transformers/main_classes/output.html#transformers.modeling_outputs.SequenceClassifierOutput\n","        # Specifically, we'll get the loss (because we provided labels) and the\n","        # \"logits\"--the model outputs prior to activation.\n","        result = model(b_input_ids, \n","                       token_type_ids=None, \n","                       attention_mask=b_input_mask, \n","                       labels=b_labels,\n","                       return_dict=True)\n","        \"\"\"\n","        if tmp_print_flag:\n","          tmp_print_flag = False\n","          print(f'result.keys() = {result.keys()}')\n","        \"\"\"\n","\n","        loss = result.loss\n","        logits = result.logits\n","\n","        \"\"\"\n","        print(f'loss {loss}')\n","        print(f'logits {logits}')\n","        \"\"\"\n","        train_preds.extend(logits.argmax(dim=1).cpu().numpy())\n","        train_targets.extend(batch[2].numpy())\n","\n","        # Accumulate the training loss over all of the batches so that we can\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value \n","        # from the tensor.\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc.\n","        optimizer.step()\n","\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","        train_acc = accuracy_score(train_targets, train_preds)\n","        train_precision = precision_score(train_targets, train_preds)\n","        train_recall = recall_score(train_targets, train_preds)\n","        train_f1 = f1_score(train_targets, train_preds)\n","\n","        io_total_train_acc += train_acc\n","        io_total_train_prec += train_precision\n","        io_total_train_recall += train_recall\n","        io_total_train_f1 += train_f1\n","\n","    io_avg_train_acc = io_total_train_acc / len(train_dataloader)\n","    io_avg_train_prec = io_total_train_prec / len(train_dataloader)\n","    io_avg_train_recall = io_total_train_recall / len(train_dataloader)\n","    io_avg_train_f1 = io_total_train_f1 / len(train_dataloader)\n","    print(\n","        f'Epoch {epoch_i+1} : \\n\\\n","        Train_acc : {io_avg_train_acc}\\n\\\n","        Train_F1 : {io_avg_train_f1}\\n\\\n","        Train_precision : {io_avg_train_prec}\\n\\\n","        Train_recall : {io_avg_train_recall}'\n","    )\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_train_loss = total_train_loss / len(train_dataloader)            \n","    \n","    # Measure how long this epoch took.\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epoch took: {:}\".format(training_time))\n","        \n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure our performance on\n","    # our validation set.\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation.\n","    model.eval()\n","\n","    valid_preds = []\n","    valid_targets = []\n","\n","    # Tracking variables \n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        \n","        # Unpack this training batch from our dataloader. \n","        #\n","        # As we unpack the batch, we'll also copy each tensor to the GPU using \n","        # the `to` method.\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids \n","        #   [1]: attention masks\n","        #   [2]: labels \n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","        \n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training).\n","        with torch.no_grad():        \n","\n","            # Forward pass, calculate logit predictions.\n","            # token_type_ids is the same as the \"segment ids\", which \n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            result = model(b_input_ids, \n","                           token_type_ids=None, \n","                           attention_mask=b_input_mask,\n","                           labels=b_labels,\n","                           return_dict=True)\n","\n","        # Get the loss and \"logits\" output by the model. The \"logits\" are the \n","        # output values prior to applying an activation function like the \n","        # softmax.\n","        loss = result.loss\n","        logits = result.logits\n","\n","        valid_preds.extend(logits.argmax(dim=1).cpu().numpy())\n","        valid_targets.extend(batch[2].numpy())\n","\n","        # Accumulate the validation loss.\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches.\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","        \n","        valid_acc = accuracy_score(valid_targets, valid_preds)\n","        valid_precision = precision_score(valid_targets, valid_preds)\n","        valid_recall = recall_score(valid_targets, valid_preds)\n","        valid_f1 = f1_score(valid_targets, valid_preds)\n","\n","        io_total_valid_acc += valid_acc\n","        io_total_valid_prec += valid_precision\n","        io_total_valid_recall += valid_recall\n","        io_total_valid_f1 += valid_f1\n","\n","    io_avg_valid_acc = io_total_valid_acc / len(validation_dataloader)\n","    io_avg_valid_prec = io_total_valid_prec / len(validation_dataloader)\n","    io_avg_valid_recall = io_total_valid_recall / len(validation_dataloader)\n","    io_avg_valid_f1 = io_total_valid_f1 / len(validation_dataloader)\n","    print(\n","            f'Epoch {epoch_i+1} : \\n\\\n","            Valid_acc : {io_avg_valid_acc}\\n\\\n","            Valid_F1 : {io_avg_valid_f1}\\n\\\n","            Valid_precision : {io_avg_valid_prec}\\n\\\n","            Valid_recall : {io_avg_valid_recall}'\n","          )\n","\n","    # Report the final accuracy for this validation run.\n","    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches.\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","    \n","    # Measure how long the validation run took.\n","    validation_time = format_time(time.time() - t0)\n","    \n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","\n","    # Record all statistics from this epoch.\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Training Accur.': io_avg_train_acc,\n","            'Training F1': io_avg_train_f1,\n","            'Training Precision': io_avg_train_prec, \n","            'Training Recall': io_avg_train_recall,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Valid. F1': io_avg_valid_f1,\n","            'Valid. Precision': io_avg_valid_prec, \n","            'Valid. Recall': io_avg_valid_recall,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"],"metadata":{"id":"wPRGelpWslTd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Analysis"],"metadata":{"id":"NA0t5b15s3vO"}},{"cell_type":"code","source":["import pandas as pd\n","\n","# Display floats with two decimal places.\n","pd.set_option('precision', 2)\n","\n","# Create a DataFrame from our training statistics.\n","df_stats = pd.DataFrame(data=training_stats)\n","\n","# Use the 'epoch' as the row index.\n","df_stats = df_stats.set_index('epoch')\n","\n","# A hack to force the column headers to wrap.\n","#df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n","\n","# Display the table.\n","df_stats"],"metadata":{"id":"XBglihEPstu8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Loss per epoch - Training VS Validation"],"metadata":{"id":"sJWJtfsDu_pN"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","\n","import seaborn as sns\n","\n","# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n","\n","# Label the plot.\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"],"metadata":{"id":"QLA624BNs1d1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Accuracy per epoch - Training VS Validation"],"metadata":{"id":"D699Sjgzua9G"}},{"cell_type":"code","source":["# Plot the learning curve.\n","plt.plot(df_stats['Training Accur.'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Accur.'], 'g-o', label=\"Validation\")\n","\n","# Label the plot.\n","plt.title(\"Training & Validation Accuracy\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"],"metadata":{"id":"BXL5MGCUuYPn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### F1 per epoch - Training VS Validation"],"metadata":{"id":"-ecPHGEQugLT"}},{"cell_type":"code","source":["# Plot the learning curve.\n","plt.plot(df_stats['Training F1'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. F1'], 'g-o', label=\"Validation\")\n","\n","# Label the plot.\n","plt.title(\"Training & Validation F1\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"F1\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"],"metadata":{"id":"prhTJjVJuljb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##### Recall per epoch - Training VS Validation"],"metadata":{"id":"TOT639Ggustl"}},{"cell_type":"code","source":["# Plot the learning curve.\n","plt.plot(df_stats['Training Recall'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Recall'], 'g-o', label=\"Validation\")\n","\n","# Label the plot.\n","plt.title(\"Training & Validation Recall\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Recall\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"],"metadata":{"id":"GyXVahVeuwHE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Save model"],"metadata":{"id":"nf7wh8C2u1jl"}},{"cell_type":"code","source":["#model_name = \"Bert4SeqClassif_augm_20220804_1249.pt\"\n","#torch.save(model.state_dict(), model_name)"],"metadata":{"id":"zKs1g-Xqu4gT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Testing the model"],"metadata":{"id":"mLx6afk3_Fjo"}},{"cell_type":"code","source":["model = BertForSequenceClassification.from_pretrained(\n","    MODEL_NAME,\n","    num_labels = NUM_CLASSES,\n","    output_attentions = False,\n","    output_hidden_states = False,\n",")\n","model.cuda()\n","\n","model.load_state_dict(torch.load('Bert4SeqClassif_augm_20220804_1249.pt'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lpg-IpVPvZL6","executionInfo":{"status":"ok","timestamp":1659631121577,"user_tz":-120,"elapsed":580,"user":{"displayName":"Isaac Olguín","userId":"09697151780329788960"}},"outputId":"846cab9b-b11d-4188-8da0-6aca4f235ca4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at nlpaueb/legal-bert-small-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-small-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["all_sentences = get_sentences(\"ToS/TestSetWithAugmentedData/SentencesBeginning/\")\n","all_labels = get_labels(\"ToS/TestSetWithAugmentedData/LabelsBeginning/\")\n","\n","input_ids = []\n","attention_masks = []\n","tmp_bool = True\n","\n","for sent in all_sentences:\n","    encoded_dict = tokenizer.encode_plus(\n","                        sent,            \n","                        add_special_tokens = True,\n","                        max_length = 512,         \n","                        pad_to_max_length = True, \n","                        return_attention_mask = True,\n","                        return_tensors = 'pt',     \n","                   )\n","    if tmp_bool:\n","      tmp_bool = False\n","      print(f'Keys of encoded_dict: {encoded_dict.keys()}')\n","       \n","    input_ids.append(encoded_dict['input_ids'])\n","    \n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(all_labels)\n","\n","test_dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","test_dataloader = DataLoader(\n","            test_dataset, \n","            sampler = RandomSampler(test_dataset),\n","            batch_size = BATCH_SIZE \n","        )"],"metadata":{"id":"3DO0p89xTIC_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ========================================\n","#               Test\n","# ========================================\n","# After the completion of each test epoch, measure our performance on\n","# our test set.\n","\n","print(\"Running Testing...\")\n","\n","t0 = time.time()\n","\n","# Put the model in evaluation mode--the dropout layers behave differently\n","# during evaluation.\n","model.eval()\n","\n","test_preds = []\n","test_targets = []\n","\n","# Tracking variables \n","total_test_accuracy = 0\n","total_test_loss = 0\n","nb_test_steps = 0\n","\n","io_total_test_acc = 0\n","io_total_test_prec = 0\n","io_total_test_recall = 0\n","io_total_test_f1 = 0\n","\n","# Evaluate data for one epoch\n","for batch in test_dataloader:\n","    \n","    # Unpack this training batch from our dataloader. \n","    #\n","    # As we unpack the batch, we'll also copy each tensor to the GPU using \n","    # the `to` method.\n","    #\n","    # `batch` contains three pytorch tensors:\n","    #   [0]: input ids \n","    #   [1]: attention masks\n","    #   [2]: labels \n","    b_input_ids = batch[0].to(device)\n","    b_input_mask = batch[1].to(device)\n","    b_labels = batch[2].to(device)\n","    \n","    # Tell pytorch not to bother with constructing the compute graph during\n","    # the forward pass, since this is only needed for backprop (training).\n","    with torch.no_grad():        \n","\n","        # Forward pass, calculate logit predictions.\n","        # token_type_ids is the same as the \"segment ids\", which \n","        # differentiates sentence 1 and 2 in 2-sentence tasks.\n","        result = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask,\n","                        labels=b_labels,\n","                        return_dict=True)\n","\n","    # Get the loss and \"logits\" output by the model. The \"logits\" are the \n","    # output values prior to applying an activation function like the \n","    # softmax.\n","    loss = result.loss\n","    logits = result.logits\n","\n","    test_preds.extend(logits.argmax(dim=1).cpu().numpy())\n","    test_targets.extend(batch[2].numpy())\n","\n","    # Accumulate the test loss.\n","    total_test_loss += loss.item()\n","\n","    # Move logits and labels to CPU\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    # Calculate the accuracy for this batch of test sentences, and\n","    # accumulate it over all batches.\n","    total_test_accuracy += flat_accuracy(logits, label_ids)\n","    \n","    test_acc = accuracy_score(test_targets, test_preds)\n","    test_precision = precision_score(test_targets, test_preds)\n","    test_recall = recall_score(test_targets, test_preds)\n","    test_f1 = f1_score(test_targets, test_preds)\n","\n","    io_total_test_acc += test_acc\n","    io_total_test_prec += test_precision\n","    io_total_test_recall += test_recall\n","    io_total_test_f1 += test_f1\n","\n","# Report the final accuracy for this test run.\n","avg_test_accuracy = total_test_accuracy / len(test_dataloader)\n","avg_test_acc = io_total_test_acc / len(test_dataloader)\n","avg_test_prec = io_total_test_prec / len(test_dataloader)\n","avg_test_recall = io_total_test_recall / len(test_dataloader)\n","avg_test_f1 = io_total_test_f1 / len(test_dataloader)\n","print(\"  =>Accuracy:  {0:.2f}\".format(avg_test_acc))\n","print(\"  =>Precision: {0:.2f}\".format(avg_test_prec))\n","print(\"  =>Recall:    {0:.2f}\".format(avg_test_recall))\n","print(\"  =>F1:        {0:.2f}\".format(avg_test_f1))\n","\n","# Calculate the average loss over all of the batches.\n","avg_test_loss = total_test_loss / len(test_dataloader)\n","\n","# Measure how long the test run took.\n","test_time = format_time(time.time() - t0)\n","\n","print(\"  Test Loss: {0:.2f}\".format(avg_test_loss))\n","print(\"  Test took: {:}\".format(test_time))\n"],"metadata":{"id":"2LmR0whpTA_C"},"execution_count":null,"outputs":[]}]}