{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NaturalAdvAttack.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "221e908ac50444b580f839bd680cf8be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eba973c29fb544908627e684c15da5f8",
              "IPY_MODEL_2ff8fa6efbfa489f8da470b048d83aac",
              "IPY_MODEL_957313dc7d174309943b8839e5b6af33"
            ],
            "layout": "IPY_MODEL_a6533b6d9ced46ddad256fcc0c306e71"
          }
        },
        "eba973c29fb544908627e684c15da5f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7496b03980ed4875a14535b3e53fdbf3",
            "placeholder": "​",
            "style": "IPY_MODEL_f110d14e3e864e6099c359f0d6874afd",
            "value": "100%"
          }
        },
        "2ff8fa6efbfa489f8da470b048d83aac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0aa019856634c30bd7b503ccbf27934",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_867f7f3dc11a4b9a9a65f3b7fbdfa849",
            "value": 1
          }
        },
        "957313dc7d174309943b8839e5b6af33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d466dce8e5f465f93d168d12d57c16c",
            "placeholder": "​",
            "style": "IPY_MODEL_9a45ab73659346829b927059c481876b",
            "value": " 1/1 [00:50&lt;00:00, 50.68s/it]"
          }
        },
        "a6533b6d9ced46ddad256fcc0c306e71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7496b03980ed4875a14535b3e53fdbf3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f110d14e3e864e6099c359f0d6874afd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0aa019856634c30bd7b503ccbf27934": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "867f7f3dc11a4b9a9a65f3b7fbdfa849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3d466dce8e5f465f93d168d12d57c16c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a45ab73659346829b927059c481876b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abyaadrafid/LDA_Lab_Defence/blob/main/NaturalAdvAttack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adversarial attacks against Legal-BERT Model (BertForSequenceClassification)"
      ],
      "metadata": {
        "id": "MXqml7sZuRKJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vgl8t7lyuGJa"
      },
      "outputs": [],
      "source": [
        "# Global variables\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "MODEL_NAME = 'nlpaueb/legal-bert-small-uncased'#'bert-base-uncased'\n",
        "EPOCHS = 3\n",
        "EMBEDDING_SIZE = 512\n",
        "NUM_CLASSES = 2\n",
        "VOCABULARY_SIZE = 30522\n",
        "NUM_TOKENS = 6\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installation of packages"
      ],
      "metadata": {
        "id": "XCxFkLyZuvz0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch-lr-finder"
      ],
      "metadata": {
        "id": "X3e7ptYOuwQl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f16249b7-91fe-40e8-bb98-4f26f601bf44"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.20.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch-lr-finder in /usr/local/lib/python3.7/dist-packages (0.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from torch-lr-finder) (3.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torch-lr-finder) (21.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-lr-finder) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-lr-finder) (1.21.6)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from torch-lr-finder) (1.12.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->torch-lr-finder) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torch-lr-finder) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torch-lr-finder) (1.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torch-lr-finder) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torch-lr-finder) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->torch-lr-finder) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "yfPJufE5vMkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import argparse\n",
        "from transformers import BertTokenizer\n",
        "from google.colab import drive\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import gc\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm_notebook"
      ],
      "metadata": {
        "id": "aPfzDo8hvPBZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Device"
      ],
      "metadata": {
        "id": "_oG87aJ3vWxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():     \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "XQKxA_5MvV0w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16ac8776-568a-4a63-b673-f788ac01bcdd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading dataset"
      ],
      "metadata": {
        "id": "9PTbIu43vb0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dsmYWRXvcPc",
        "outputId": "602bb5a1-3132-4569-dd98-2ed2b97913f0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append(\"/content/drive/MyDrive/Colab Notebooks\")"
      ],
      "metadata": {
        "id": "s1Xx7Fk6wPwh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ARAE_utils import Seq2Seq, MLP_D, MLP_G, generate\n",
        "from attack_util import project_noise, one_hot_prob, GPT2_LM_loss, select_fluent_trigger"
      ],
      "metadata": {
        "id": "xdkyXtcIw_S9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Funtion to read all sentences\n",
        "def get_sentences(path):\n",
        "    sentences= []\n",
        "    for filename in os.listdir(path):\n",
        "        with open(path+filename, 'r') as f:\n",
        "            for sentence in f :\n",
        "                sentences.append(sentence)\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "knj4Vy1wwsfI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read get all labels\n",
        "def get_labels(path):\n",
        "    all_labels = []\n",
        "    for filename in os.listdir(path):\n",
        "        file_labels = []\n",
        "        with open(path+filename, 'r') as f:\n",
        "            for label in f :\n",
        "                all_labels.append(int(label))\n",
        "    return all_labels"
      ],
      "metadata": {
        "id": "utKztVafwtnw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading sentences and labels\n",
        "all_sentences = get_sentences(\"Sentences/\")\n",
        "all_labels = get_labels(\"Labels/\")"
      ],
      "metadata": {
        "id": "mkp9MZKewxDN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since unfair sentences are marked as \"-1\", we change them to \"0\" for simplicity. Zero means fair, One means unfair\n",
        "all_labels =  [0 if label ==-1 else label for label in all_labels]"
      ],
      "metadata": {
        "id": "bnor58FKwxy2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bert Tokenizer"
      ],
      "metadata": {
        "id": "jU5yamL5xKAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=True) # the model 'bert-base-uncased' only contains lower case sentences"
      ],
      "metadata": {
        "id": "xMiwR7ldxLwC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c92cd5ad-ee08-44b9-c70a-e01f7125bb66"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT tokenizer...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model BertForSequenceClassification (Load model)"
      ],
      "metadata": {
        "id": "JpohQx5xyqwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels = NUM_CLASSES,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "model.to(device);"
      ],
      "metadata": {
        "id": "tkpyAA69yuEO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7316937-6f30-4359-f9e5-79c7a0059a9b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at nlpaueb/legal-bert-small-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at nlpaueb/legal-bert-small-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.load_state_dict(torch.load('Bert4SeqClassif_202207072015.pt'))"
      ],
      "metadata": {
        "id": "Q-aGx3Q60e4w"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_ARAE_models(load_path, args):\n",
        "    # function to load ARAE model.\n",
        "    if not os.path.exists(load_path):\n",
        "        print('Please download the pretrained ARAE model first')\n",
        "        \n",
        "    ARAE_args = json.load(open(os.path.join(load_path, 'options.json'), 'r'))\n",
        "    vars(args).update(ARAE_args)\n",
        "    autoencoder = Seq2Seq(emsize=args.emsize,\n",
        "                          nhidden=args.nhidden,\n",
        "                          ntokens=args.ntokens,\n",
        "                          nlayers=args.nlayers,\n",
        "                          noise_r=args.noise_r,\n",
        "                          hidden_init=args.hidden_init,\n",
        "                          dropout=args.dropout,\n",
        "                          gpu=True)\n",
        "    gan_gen = MLP_G(ninput=args.z_size, noutput=args.nhidden, layers=args.arch_g)\n",
        "    gan_disc = MLP_D(ninput=args.nhidden, noutput=1, layers=args.arch_d)\n",
        "\n",
        "    autoencoder = autoencoder.cuda()\n",
        "    gan_gen = gan_gen.cuda()\n",
        "    gan_disc = gan_disc.cuda()\n",
        "\n",
        "    ARAE_word2idx = json.load(open(os.path.join(args.load_path, 'vocab.json'), 'r'))\n",
        "    ARAE_idx2word = {v: k for k, v in ARAE_word2idx.items()}\n",
        "\n",
        "    print('Loading models from {}'.format(args.load_path))\n",
        "    loaded = torch.load(os.path.join(args.load_path, \"model.pt\"))\n",
        "    autoencoder.load_state_dict(loaded.get('ae'))\n",
        "    gan_gen.load_state_dict(loaded.get('gan_g'))\n",
        "    gan_disc.load_state_dict(loaded.get('gan_d'))\n",
        "    return ARAE_args, ARAE_idx2word, ARAE_word2idx, autoencoder, gan_gen, gan_disc"
      ],
      "metadata": {
        "id": "MDfEdcbSxGHZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--load_path', type=str, default='/content/drive/MyDrive/oneb_pretrained',\n",
        "                    help='directory to load models from')\n",
        "parser.add_argument('--seed', type=int, default=1111,\n",
        "                    help='random seed')\n",
        "parser.add_argument('--sample', action='store_true',\n",
        "                    help='sample when decoding for generation')\n",
        "parser.add_argument('--len_lim', type=int, default=5,\n",
        "                    help='maximum length of sentence')\n",
        "parser.add_argument('--r_lim', type=float, default=1,\n",
        "                    help='lim of radius of z')\n",
        "parser.add_argument('--sentiment_path', type=str, default='./opinion_lexicon_English',\n",
        "                    help='directory to load sentiment word from')\n",
        "parser.add_argument('--z_seed', type=float, default=6.,\n",
        "                    help='noise seed for z')\n",
        "parser.add_argument('--avoid_l', type=int, default=4,\n",
        "                    help='length to avoid repeated pattern')\n",
        "parser.add_argument('--lr', type=float, default=1e3,\n",
        "                    help='learn rate')\n",
        "parser.add_argument('--attack_class', type=str, default='1',\n",
        "                    help='the class label to attack')\n",
        "parser.add_argument('--noise_n', type=int, default=1,\n",
        "                    help='number of generated noise vectors')\n",
        "parser.add_argument('--tot_runs', type=int, default=1,\n",
        "                    help='number of attack runs')\n",
        "args = parser.parse_args([])"
      ],
      "metadata": {
        "id": "Pry0DuSkxLp_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r_threshold = args.r_lim\n",
        "step_bound = r_threshold / 100\n",
        "max_iterations = 1000"
      ],
      "metadata": {
        "id": "sFStUzc-xsvU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(args.seed)\n",
        "np.random.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "torch.cuda.manual_seed(args.seed)\n",
        "\n",
        "# initialize ARAE model.\n",
        "ARAE_args, ARAE_idx2word, ARAE_word2idx, autoencoder, gan_gen, gan_disc = load_ARAE_models(args.load_path, args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwEr6EkjxXcJ",
        "outputId": "20bc721b-3ad5-422f-dd25-0b4b35aee7ad"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading models from /content/drive/MyDrive/oneb_pretrained\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# returns the wordpiece embedding weight matrix\n",
        "def get_embedding_weight(language_model):\n",
        "    for module in language_model.modules():\n",
        "        if isinstance(module, torch.nn.Embedding):\n",
        "            if module.weight.shape[0] == 30522:\n",
        "                return module.weight.detach()"
      ],
      "metadata": {
        "id": "1MV3isar2dvF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add hooks for embeddings\n",
        "def add_hooks(language_model):\n",
        "    for module in language_model.modules():\n",
        "        if isinstance(module, torch.nn.Embedding):\n",
        "            if module.weight.shape[0] == 30522:\n",
        "                module.weight.requires_grad = True\n",
        "                module.register_full_backward_hook(extract_grad_hook)"
      ],
      "metadata": {
        "id": "ymN2vLUT6Oe5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hook used in add_hooks()\n",
        "extracted_grads = []\n",
        "def extract_grad_hook(module, grad_in, grad_out):\n",
        "    extracted_grads.append(grad_out[0])"
      ],
      "metadata": {
        "id": "8JjcRhGE6hUc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model.to(device)\n",
        "\n",
        "add_hooks(model) # add gradient hooks to embeddings\n",
        "embedding_weight = get_embedding_weight(model) # save the word embedding matrix\n"
      ],
      "metadata": {
        "id": "SG4w1AElxpNR"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ARAE_weight_embedding = []\n",
        "for num in range(len(ARAE_idx2word)):\n",
        "    ARAE_weight_embedding.append(embedding_weight[tokenizer.convert_tokens_to_ids(ARAE_idx2word[num])])\n",
        "ARAE_weight_embedding = torch.stack(ARAE_weight_embedding)\n",
        "print(ARAE_weight_embedding.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy1dQfxCyKjf",
        "outputId": "9ea0fa06-638e-4fe6-d1de-ab9cac70078d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([30004, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trigger generation"
      ],
      "metadata": {
        "id": "JIyze6jK2bpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### General functions"
      ],
      "metadata": {
        "id": "mLTLH3AJ5-Lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creates the batch of target texts with -1 placed at the end of the sequences for padding (for masking out the loss).\n",
        "def make_target_batch(tokenizer, device, target_texts):\n",
        "    encoded_texts = []\n",
        "    max_len = 0\n",
        "    for target_text in target_texts:\n",
        "        encoded_target_text = tokenizer.encode_plus(\n",
        "            target_text,\n",
        "            add_special_tokens = True,\n",
        "            max_length = EMBEDDING_SIZE - NUM_TOKENS,\n",
        "            pad_to_max_length = True,\n",
        "            return_attention_mask = True\n",
        "        )\n",
        "        encoded_texts.append(encoded_target_text.input_ids)\n",
        "        if len(encoded_target_text.input_ids) > max_len:\n",
        "            max_len = len(encoded_target_text)\n",
        "\n",
        "    for indx, encoded_text in enumerate(encoded_texts):\n",
        "        if len(encoded_text) < max_len:\n",
        "            encoded_texts[indx].extend([-1] * (max_len - len(encoded_text)))\n",
        "\n",
        "    target_tokens_batch = None\n",
        "    for encoded_text in encoded_texts:\n",
        "        target_tokens = torch.tensor(encoded_text, device=device, dtype=torch.long).unsqueeze(0)\n",
        "        if target_tokens_batch is None:\n",
        "            target_tokens_batch = target_tokens\n",
        "        else:\n",
        "            target_tokens_batch = torch.cat((target_tokens, target_tokens_batch), dim=0)\n",
        "    return target_tokens_batch"
      ],
      "metadata": {
        "id": "73ZQsJW_6z3h"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_input_masks_and_labels_with_tokens(sentences, labels, tokens):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for sent in sentences:\n",
        "        sent_with_tokens = \" \".join(tokens) + \" \" + sent\n",
        "\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                        sent,\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 512 - NUM_TOKENS+1,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt',\n",
        "                   )\n",
        "           \n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    input_ids = torch.cat(input_ids, dim=0)\n",
        "    attention_masks = torch.cat(attention_masks, dim=0)\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    return input_ids, attention_masks, labels"
      ],
      "metadata": {
        "id": "lV7lkCZP731g"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_loss_and_metrics(model, dataloader, device):\n",
        "    # get initial loss for the trigger\n",
        "    model.zero_grad()\n",
        "\n",
        "    test_preds = []\n",
        "    test_targets = []\n",
        "\n",
        "    # Tracking variables \n",
        "    total_test_accuracy = 0\n",
        "    total_test_loss = 0\n",
        "    io_total_test_acc = 0\n",
        "    io_total_test_prec = 0\n",
        "    io_total_test_recall = 0\n",
        "    io_total_test_f1 = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        result = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels,\n",
        "                    return_dict=True)\n",
        "\n",
        "        loss = result.loss\n",
        "        logits = result.logits\n",
        "\n",
        "        test_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
        "        test_targets.extend(batch[2].cpu().numpy())\n",
        "\n",
        "        # Accumulate the validation loss.\n",
        "        total_test_loss += loss.item()\n",
        "\n",
        "        test_preds.extend(logits.argmax(dim=1).cpu().numpy())\n",
        "        test_targets.extend(batch[2].cpu().numpy())\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        loss.backward()        \n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.        \n",
        "        test_acc = accuracy_score(test_targets, test_preds)\n",
        "        test_precision = precision_score(test_targets, test_preds)\n",
        "        test_recall = recall_score(test_targets, test_preds)\n",
        "        test_f1 = f1_score(test_targets, test_preds)\n",
        "\n",
        "        io_total_test_acc += test_acc\n",
        "        io_total_test_prec += test_precision\n",
        "        io_total_test_recall += test_recall\n",
        "        io_total_test_f1 += test_f1\n",
        "\n",
        "    io_avg_test_loss = total_test_loss/len(dataloader)\n",
        "    io_avg_test_acc = io_total_test_acc / len(dataloader)\n",
        "    io_avg_test_prec = io_total_test_prec / len(dataloader)\n",
        "    io_avg_test_recall = io_total_test_recall / len(dataloader)\n",
        "    io_avg_test_f1 = io_total_test_f1 / len(dataloader)\n",
        "    print(\n",
        "            f'Loss {io_avg_test_loss} : \\t\\\n",
        "            Valid_acc : {io_avg_test_acc}\\t\\\n",
        "            Valid_F1 : {io_avg_test_f1}\\t\\\n",
        "            Valid_precision : {io_avg_test_prec}\\t\\\n",
        "            Valid_recall : {io_avg_test_recall}'\n",
        "          )\n",
        "\n",
        "    return io_avg_test_loss, io_avg_test_acc, io_avg_test_prec, io_avg_test_recall, io_avg_test_f1"
      ],
      "metadata": {
        "id": "myWBJ3tc-XCR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def change_input_ids_with_candidate_token(input_ids, position, candidate):\n",
        "    input_ids[:,position] = candidate\n",
        "\n",
        "    return input_ids"
      ],
      "metadata": {
        "id": "o-ZoFvcJXsH5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "metadata": {
        "id": "NfnLkgMPgMui"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positions_unfair = np.where(np.array(all_labels) == 1)[0]\n",
        "print(f'First 32 positions: {positions_unfair[0:32]} with total of unfair sentences {len(positions_unfair)}')\n",
        "\n",
        "target_unfair_sentences = []\n",
        "labels_unfair_sentences = []\n",
        "for index in range(len(positions_unfair)):\n",
        "    target_unfair_sentences.append(all_sentences[positions_unfair[index]])\n",
        "    labels_unfair_sentences.append(all_labels[positions_unfair[index]])\n"
      ],
      "metadata": {
        "id": "cr3D9l6q8CD5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8fda17b-1740-41af-965f-a5991cf2c0ea"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 32 positions: [ 11  44  48  57  58  59  75  80  89  90  93  95  96 112 113 116 144 145\n",
            " 148 151 163 183 184 195 196 197 198 199 200 209 210 211] with total of unfair sentences 1032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trigger_tokens = [0,0,0,0,0,0]"
      ],
      "metadata": {
        "id": "VajCNxNLAUmN"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, attention_masks, labels = get_input_masks_and_labels_with_tokens(target_unfair_sentences, labels_unfair_sentences, tokenizer.decode(trigger_tokens))\n",
        "\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "LuqGPIZ9bsM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dedf6806-3143-46ba-aaa5-531662eca5d5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2307: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqDDvGjrNCX3",
        "outputId": "3d7c5e94-2be0-4bfd-ca96-fe510d390c3c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 512, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 512)\n",
              "      (token_type_embeddings): Embedding(2, 512)\n",
              "      (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (key): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (value): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "              (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
              "            (LayerNorm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=512, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_generated_sentences_from_ARAE(max_indices):\n",
        "  max_indices = max_indices.data.cpu().numpy()\n",
        "  sentences = []\n",
        "  for idx in max_indices:\n",
        "      # generated sentence\n",
        "      words = tokenizer.convert_ids_to_tokens(idx)\n",
        "      # truncate sentences to first occurrence of <eos>\n",
        "      truncated_sent = []\n",
        "      for w in words:\n",
        "          if w != '<eos>':\n",
        "              truncated_sent.append(w)\n",
        "          else:\n",
        "              break\n",
        "      sent = \" \".join(truncated_sent)\n",
        "      sentences.append(sent)\n",
        "  print(sentences)"
      ],
      "metadata": {
        "id": "iQwvA0W8rdbZ"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GlueEncoder(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.layer = nn.Linear(256,1)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = x.permute(1,2,0)\n",
        "    return self.layer(x).squeeze(-1)"
      ],
      "metadata": {
        "id": "Ffd_gJSB1l5L"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Pw8wfig0GmCb"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_with_trigger(out_emb, tokens, masks, labels):\n",
        "  model.train()\n",
        "  token_embeddings = model.bert.embeddings.word_embeddings(tokens)\n",
        "  out_emb = out_emb.repeat(token_embeddings.shape[0],1,1)\n",
        "  input_embeddings = torch.cat([token_embeddings, out_emb], dim = 1)\n",
        "  enc_output = model.bert.encoder(input_embeddings)\n",
        "  pooler_output = model.bert.pooler(enc_output.last_hidden_state)\n",
        "  dropout_output = model.dropout(pooler_output)\n",
        "  return model.classifier(dropout_output)"
      ],
      "metadata": {
        "id": "ZIsezPCS6s_8"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_loss_and_metrics(model, dataloader, device)\n",
        "maxlen = args.len_lim\n",
        "# initialize noise\n",
        "noise_n = args.noise_n  # this should be a factor of batch_size\n",
        "tot_runs = args.tot_runs\n",
        "n_repeat = 1\n",
        "\n",
        "\n",
        "r_threshold = args.r_lim\n",
        "step_bound = r_threshold / 100\n",
        "max_iterations = 1000\n",
        "\n",
        "patience_lim = 3\n",
        "patience = 0 \n",
        "max_trial = 3\n",
        "all_output = list()\n",
        "log_loss = int(1e2)\n",
        "\n",
        "for tmp in tqdm_notebook(range(tot_runs)):\n",
        "    get_loss_and_metrics(model, dataloader, device)\n",
        "    step_size = args.lr\n",
        "    step_scale = 0.1 \n",
        "    patience = 0\n",
        "    old_noise = None\n",
        "    old_loss = float('-Inf')\n",
        "    loss_list = list()\n",
        "    update = False\n",
        "    i_trial = 0\n",
        "\n",
        "    torch.manual_seed(args.z_seed + tmp)\n",
        "    print('z_seed:{}'.format(args.z_seed + tmp))\n",
        "    noise = torch.randn(noise_n, ARAE_args['z_size'], requires_grad=True, device = \"cuda\")\n",
        "    noise = Variable(noise, requires_grad=True)\n",
        "    start_noise_data = noise.data.clone()\n",
        "    iter = 0\n",
        "    for i, batch in enumerate(dataloader) :\n",
        "        # evaluate_batch(model, batch, trigger_token_ids, snli)\n",
        "        # generate sentence with ARAE, output the word embedding instead of index.\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.train()\n",
        "        autoencoder.train()\n",
        "        gan_gen.eval()\n",
        "        gan_disc.eval()\n",
        "\n",
        "        hidden = gan_gen(noise)\n",
        "\n",
        "        max_indices, decoded = autoencoder.generate_decoding(hidden=hidden, maxlen=maxlen, sample=False, avoid_l=args.avoid_l)\n",
        "        \n",
        "        # print_generated_sentences_from_ARAE(max_indices)\n",
        "        \n",
        "        decoded = torch.stack(decoded, dim=1).float()\n",
        "        if n_repeat > 1:\n",
        "            decoded = torch.repeat_interleave(decoded, repeats=n_repeat, dim=0)\n",
        "\n",
        "        decoded_prob = F.softmax(decoded, dim=-1)\n",
        "        decoded_prob = one_hot_prob(decoded_prob, max_indices)\n",
        "        out_emb = torch.matmul(decoded_prob, ARAE_weight_embedding)\n",
        "\n",
        "        output = forward_with_trigger(out_emb, b_input_ids, b_input_mask, b_labels.unsqueeze(-1))\n",
        "\n",
        "        oh_targets = F.one_hot(b_labels, num_classes=2).to(torch.float32).to(device)\n",
        "        loss = criterion(output, oh_targets)\n",
        "        iter += 1\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        if noise.grad is not None:\n",
        "          noise.grad.zero_()\n",
        "        noise.retain_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        noise_diff = step_size * noise.grad.data\n",
        "        noise_diff = project_noise(noise_diff, r_threshold=step_bound)\n",
        "\n",
        "        noise.data = noise.data + noise_diff\n",
        "\n",
        "        whole_diff = noise.data - start_noise_data\n",
        "        whole_diff = project_noise(whole_diff, r_threshold=r_threshold)\n",
        "        noise.data = start_noise_data + whole_diff\n",
        "\n",
        "        if iter % log_loss == 0:\n",
        "            cur_loss = np.mean(loss_list)\n",
        "            print('current iter:{}'.format(iter))\n",
        "            print('current loss:{}'.format(cur_loss))\n",
        "\n",
        "            loss_list = list()\n",
        "            if cur_loss > old_loss:\n",
        "                patience = 0\n",
        "                old_loss = cur_loss\n",
        "                old_noise = noise.data.clone()\n",
        "                update = True\n",
        "            else:\n",
        "                patience += 1\n",
        "\n",
        "            print('current patience:{}'.format(patience))\n",
        "            print('\\n')\n",
        "\n",
        "            if patience >= patience_lim:\n",
        "                patience = 0\n",
        "                step_size *= step_scale\n",
        "                noise.data = old_noise\n",
        "                print('current step size:{}'.format(step_size))\n",
        "                i_trial += 1\n",
        "                print('current trial:{}'.format(i_trial))\n",
        "                print('\\n')\n",
        "\n",
        "        if i_trial >= max_trial or iter >= max_iterations:\n",
        "            if update:\n",
        "                with torch.no_grad():\n",
        "                    noise_new = torch.ones(noise_n, ARAE_args['z_size'], requires_grad=False).cuda()\n",
        "                    noise_new.data = old_noise\n",
        "                    hidden = gan_gen(noise_new)  # [:1, :]\n",
        "                    max_indices, decoded = autoencoder.generate_decoding(hidden=hidden, maxlen=maxlen, sample=False, avoid_l=args.avoid_l)\n",
        "\n",
        "                    decoded = torch.stack(decoded, dim=1).float()\n",
        "                    if n_repeat > 1:\n",
        "                        decoded = torch.repeat_interleave(decoded, repeats=n_repeat, dim=0)\n",
        "\n",
        "                    decoded_prob = F.softmax(decoded, dim=-1)\n",
        "                    decoded_prob = one_hot_prob(decoded_prob, max_indices)\n",
        "\n",
        "                sen_idxs = torch.argmax(decoded_prob, dim=2)\n",
        "                sen_idxs = sen_idxs.cpu().numpy()\n",
        "\n",
        "                output_s = list()\n",
        "                glue = ' '\n",
        "                sentence_list = list()\n",
        "                for ss in sen_idxs:\n",
        "                    sentence = [ARAE_idx2word[s] for s in ss]\n",
        "                    trigger_token_ids = list()\n",
        "                    last_word = None\n",
        "                    last_word2 = None\n",
        "                    contain_sentiment_word = False\n",
        "                    new_sentence = list()\n",
        "                    for word in sentence:\n",
        "                        cur_idx = tokenizer.convert_tokens_to_ids(word)\n",
        "                        if cur_idx != last_word and cur_idx != last_word2:\n",
        "                            trigger_token_ids.append(cur_idx)\n",
        "                            new_sentence.append(word)\n",
        "                            last_word2 = last_word\n",
        "                            last_word = cur_idx\n",
        "\n",
        "                    threshold = 0.5\n",
        "                    num_lim = 20\n",
        "                    s_str = glue.join(new_sentence)\n",
        "                    if not (s_str in sentence_list):\n",
        "                        _, accuracy, _, _ ,_ = get_loss_and_metrics(model, dataloader, device)\n",
        "                        if accuracy < threshold:\n",
        "                            sentence_list.append(s_str)\n",
        "                            output_s.append((s_str, accuracy, contain_sentiment_word))\n",
        "\n",
        "                if len(output_s) > 0:\n",
        "                    all_output = all_output + output_s\n",
        "                update = False\n",
        "            break"
      ],
      "metadata": {
        "id": "pdhC5LNWQk_x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "221e908ac50444b580f839bd680cf8be",
            "eba973c29fb544908627e684c15da5f8",
            "2ff8fa6efbfa489f8da470b048d83aac",
            "957313dc7d174309943b8839e5b6af33",
            "a6533b6d9ced46ddad256fcc0c306e71",
            "7496b03980ed4875a14535b3e53fdbf3",
            "f110d14e3e864e6099c359f0d6874afd",
            "c0aa019856634c30bd7b503ccbf27934",
            "867f7f3dc11a4b9a9a65f3b7fbdfa849",
            "3d466dce8e5f465f93d168d12d57c16c",
            "9a45ab73659346829b927059c481876b"
          ]
        },
        "outputId": "020e08b7-bafe-4c01-f768-01ad2b0f66ed"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss 0.9242930769920349 : \t            Valid_acc : 0.06792916966156409\t            Valid_F1 : 0.12703944889626015\t            Valid_precision : 1.0\t            Valid_recall : 0.06792916966156409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "221e908ac50444b580f839bd680cf8be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss 0.9315606181438153 : \t            Valid_acc : 0.07304722376856175\t            Valid_F1 : 0.13610433909229983\t            Valid_precision : 1.0\t            Valid_recall : 0.07304722376856175\n",
            "z_seed:6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_loss_and_metrics(model, dataloader, device)"
      ],
      "metadata": {
        "id": "LVY4cq-qApfw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ada4762-8384-438c-9427-b3b53b89414f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss 0.918743673654703 : \t            Valid_acc : 0.0819289312100446\t            Valid_F1 : 0.1510989260208236\t            Valid_precision : 1.0\t            Valid_recall : 0.0819289312100446\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.918743673654703,\n",
              " 0.0819289312100446,\n",
              " 1.0,\n",
              " 0.0819289312100446,\n",
              " 0.1510989260208236)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zc-XQYBBCKd3"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}